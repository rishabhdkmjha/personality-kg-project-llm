# ==============================================================================
# LLM-Powered Personality Knowledge Graph Construction - Full Pipeline
# ==============================================================================
# This single cell contains the complete end-to-end workflow.
# It will:
# 1. Install all necessary Python libraries.
# 2. Check for and confirm GPU availability.
# 3. Install and start the Ollama server in the background.
# 4. Pull the Llama 3 model to be used for extraction.
# 5. Define the knowledge graph's ontology (schema).
# 6. Process a sample text using the LLM to extract nodes and relationships.
# 7. Connect to a Neo4j AuraDB cloud database.
# 8. Populate the database with the extracted knowledge graph.
#
#  ACTION REQUIRED:
#   - Ensure your Colab runtime is set to "T4 GPU".
#   - Replace the placeholder Neo4j AuraDB credentials in Step 5.
# ==============================================================================

import os
import subprocess
import time
from IPython.display import display, Markdown

# --- Step 0: Initial Setup and GPU Check ---

def print_step(step, message):
    display(Markdown(f"### {step}"))
    print(message)

print_step("Step 0: Initial Setup", "Installing required libraries...")
# Use -q for a quieter installation
!pip install -U -q langchain langchain-experimental langchain-community langchain-neo4j neo4j langchain-ollama

print("\nChecking for GPU...")
# This command will fail if no GPU is found, which is a good indicator.
try:
    subprocess.run(['nvidia-smi'], check=True)
    print(" GPU is active.")
except FileNotFoundError:
    print(" WARNING: 'nvidia-smi' command not found. A GPU is required for this notebook to run efficiently.")
    print("Please go to Runtime -> Change runtime type and select 'T4 GPU'.")


# --- Step 1: Install and Start the Ollama Server ---
# This step is crucial. It sets up the local LLM server within the Colab environment.
# Running this on a free tier Colab instance is possible.
print_step("Step 1: Ollama Setup", "Installing and starting Ollama server...")

# Install Ollama using the official script
!curl -fsSL https://ollama.com/install.sh | sh

# Start the Ollama server as a background process
# This allows the notebook to continue running while the server is active.
ollama_process = subprocess.Popen(['ollama', 'serve'])
print("Starting Ollama server in the background...")

# Give the server a moment to initialize
time.sleep(10)
print("Ollama server should be running.")

# Pull the Llama 3 model. This will download the model to the Colab instance.
!ollama pull llama3

print("\n---  Ollama server is running and Llama3 model is ready. ---")


# --- Step 2: Import Libraries for the Main Pipeline ---
print_step("Step 2: Importing Libraries", "Importing LangChain and Neo4j components...")

from langchain_ollama import OllamaLLM
from langchain_experimental.graph_transformers import LLMGraphTransformer
from langchain_core.documents import Document
from langchain_neo4j import Neo4jGraph

print("Libraries imported successfully.")


# --- Step 3: Define Ontology and Initialize Transformers ---
print_step("Step 3: Defining Ontology", "Initializing the LLM and Graph Transformer...")

# Initialize the Ollama LLM. This will connect to the server we just started.
llm = OllamaLLM(model="llama3")

# Define the ontology (the schema for our graph).
# These are the specific types of nodes and relationships the LLM is allowed to extract.
# This provides crucial guidance and constraint to the model.
allowed_nodes =

allowed_relationships =

# Initialize the LLMGraphTransformer with the defined schema
llm_transformer = LLMGraphTransformer(
    llm=llm,
    allowed_nodes=allowed_nodes,
    allowed_relationships=allowed_relationships
)

print("LLM and Graph Transformer initialized.")


# --- Step 4: Process Text and Extract Graph Data ---
print_step("Step 4: Extracting Knowledge", "Processing text to extract graph data...")

# Here is our sample text, similar to the one from our synthetic data.
# Good quality KGs are often produced from paragraphs of 200-300 words.[1]
synthetic_text = """
Alice was known for her meticulous planning and dutiful nature. She would always finish tasks right away,
a trait that her colleague, Bob, found both admirable and intimidating. Unlike Alice,
Bob was a free spirit, always suggesting spontaneous trips and new, unproven ideas, showing great imagination.
Alice's behavior is a clear indicator of conscientiousness, specifically the facet of self-discipline.
"""

# Wrap the text in a LangChain Document object
documents =

# Use the transformer to convert the text into graph-structured data.
# This is where the LLM does the heavy lifting of extraction.
graph_documents = llm_transformer.convert_to_graph_documents(documents)

print("\n--- Extracted Graph Data ---")
print(graph_documents)


# --- Step 5: Populate the Neo4j Graph Database ---
print_step("Step 5: Populating Database", "Connecting to Neo4j and adding graph data...")

# ---  IMPORTANT: UPDATE THESE THREE LINES WITH YOUR AURA DB CREDENTIALS ---
# Replace the placeholder values with your actual credentials from your Neo4j AuraDB instance.
os.environ = "neo4j+s://xxxxxxxx.databases.neo4j.io"  # Replace with your AuraDB URI
os.environ = "neo4j"
os.environ = "your_aura_db_password"          # Replace with your AuraDB password

# Initialize the Neo4jGraph connection object
try:
    graph = Neo4jGraph()
    # Clear the existing graph to start fresh for this run
    graph.query("MATCH (n) DETACH DELETE n")

    # Add the extracted graph documents to the database.
    graph.add_graph_documents(graph_documents)

    print("\n Knowledge graph populated successfully!")
    print("You can now open your Neo4j Aura console to visualize the graph.")

except Exception as e:
    print(f"\n An error occurred while connecting to or populating Neo4j: {e}")
    print("Please double-check your Neo4j AuraDB credentials and ensure the database is running.")

# --- End of Pipeline ---
print_step("Pipeline Complete", "You can now explore your knowledge graph in Neo4j.")
